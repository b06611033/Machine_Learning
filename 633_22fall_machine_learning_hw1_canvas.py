# -*- coding: utf-8 -*-
"""633_22Fall_Machine_Learning_HW1_CANVAS.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wmEcCZH3TsHI2DdAJh57jwE-4bmc35S_

# CSCE633 Spring 2022 (Total 100 pts)

**Machine Learning**

**Homework 1**

Instructor: Yoonsuck Choe

Aug 31, 2022


---

**Student name:** Lastname, Firstname

Hou, Po Han
---

# Common instructions (Read carefully)

**Submission:**

1. After you complete each section, "Save and pin" the revision. (**File -> Save and pin revision**). 
> There will be a 20-point penalty for not including revision history in the submission as instructed below.
2. When you're ready to submit, 
> 1. download the ipynb file (**File -> Download .ipynb**).
> 2. take screenshot of revision history, by comparing each successive pinned revisions (there will be several of these: final vs. latest pin, latest pin vs. previous pin, etc.). (**File -> Revision history**, then click on the radio buttons for pinned revisions to compare. Scroll down to the part that shows the major revised part, and take a screenshot. No need to show the entire revision history.)
>> Important: Unselect "[ ] Show output" to suppress diff of execution outputs.
>> Jupyter notebook: If you're using Jupyter notebook on your local machine, run %history -g -f filename.log  and submit the log file. 
> 3. submit a zip file **lastname-firstname.zip** containing the ipynb file and all revision screenshots.

**Using the markdown language in the "text" boxes:**

Note: See https://colab.research.google.com/notebooks/markdown_guide.ipynb#scrollTo=70pYkR9LiOV0 for how to use the markdown when writing your answer in the text boxes.

# Section I. Supervised Learning (15 pt)

**Problem 1 (Written: 6 pts):**
Theorem: [Haussler, 1988] If the hypothesis space $H$ is finite, and $D$ is a sequence of $m \ge 1$ independent random examples of some target concept $c$, then for any $0 \le \epsilon \le 1$, the probability that the version space with respect to $H$ and $D$ is not $\epsilon$-exhausted (with respect to c) is less than

$$|H|e^{-\epsilon m}$$

This bounds the probability that any consistent learner will output a hypothesis $h$ with $error(h) \ge \epsilon$. If we want this probability to be below $\delta$

$$|H|e^{-\epsilon m} \le \delta \tag{1}$$  

then

$$m \ge \frac{1}{\epsilon}(ln(H) + ln(\frac{1}{\delta})) \tag{2}.$$

Discuss how m increases or decreases as $\epsilon$ and $\delta$ change.

> - | Mark one choice | Discuss the implications
>--- | --- | --- 
>If $\epsilon$ increases | then $m$ will ($\uparrow$,  $\downarrow)$ |
>If $\delta$ increases | then $m$ will ($\uparrow$, $\downarrow$) |
>Given a fixed $m$, if $\delta$ decreases | then $\epsilon$ must either ($\uparrow$,  $\downarrow$) |

**Answer:**

Write your answer here. You can either type in the answer below (for math equations, use LaTex syntax [example below]), or you can insert a scan or photo of your handwritten solution. 

Example equation: $ y = x^2 + 5 + \frac{3}{4} $


(1)A single unit perceptron can't correctly classify this training set, because the data can't be separated by single line.

(2)Two and three units also can't classify it correctly. As the image shows, it takes four lines to separate the two classes, so four or more units can classify it correctly
"""

# You can type in the code here for plotting the data above. Replace the array below with the data in the array above. 
#
# Make sure how you understand how to  turn the table above into a plot like this, manually.
# Here's an example: 
# x  y   c
# -  -  --- 
# 0  0  0  ---> x,y coordinate = (0, 0) : mark this as "x"
# 1  5  1  ---> x,y coordinate = (1, 5) : mark this as "o"
# 2  1  0  ... etc
# 3  2  1

# import some necessary packages 
import numpy as np
import matplotlib.pyplot as plt


x = np.array([0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4])
y = np.array([0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4])
c = np.array([0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0])
plt.plot(x[c==1],y[c==1],"o",x[c==0],y[c==0],"x")

"""# Section III. Gradient Descent (30 pt)

**Problem 4 (Written: 5 pts):** Given an error function 
$$E(w) = \frac{1}{40}w(w+1)(w-8)(w-10),$$
find:
$$\frac{\partial E(w)}{\partial w}.$$
Note that this is simply an ordinary derivative $\frac{dE}{dw}$. Do not use Wolfram Alpha or similar package.

**Answer:**
$\frac{\partial E(w)}{\partial w}$ = $\frac{\partial\frac{1}{40}(w^4 - 17w^3 + 62w^2 + 80w)}{\partial w}$ = $\frac{1}{40}(4w^3 - 51w^2 + 124w + 80)$
 = $ \frac{1}{10}w^3-\frac{51}{40}w^2+\frac{31}{10}w+2$

**Problem 5 (Written: 5 pts):**
(1) How many minima does $E(w)$ have? You can draw a rough plot from $w=-4$ to $12$. (2) Which is/are the local minimum/minima that isn't/aren't a global minimum? (3) How can we set the value of initial $w$, so that $w$ can converge to the local minimum/minima discussed in (2)?
"""

# Example: modify the function
def E_test(w):
  return 1/40*(w*w*w*w - 17*w*w*w + 62*w*w + 80*w)

w = np.linspace(-4,12,50)

# IMPLEMENT : for the problem 4 function, use the following range:
# x = np.linspace(-4,12,50)

plt.plot(w,E_test(w),"*-")

"""**Answer:**

(1) 2

(2) Approximately w = -0.5 is the local minimum that is not global minimum

(3) We can set w <= 4, then w will decent to the local minimum approximately -0.5

**Problem 6 (Written: 5 pts):** With $\frac{\partial E(w)}{\partial w}$ calculated above, if you want to adjust $w$ to minimize $E(w)$, what should $\Delta w$ be (include the learning rate $\eta$)? Write the answer as a polynomial function of $w$. (This is trivial, given the answer to problem 4. Be careful with the sign.)

Answer: 

$\Delta w$ = $-\eta(\frac{1}{10}w^3-\frac{51}{40}w^2+\frac{31}{10}w+2)$

**Problem 7 (Program: 10 pts):**
Using the gradient found above, write a short program to

1. Initialize $w$ to a particular value $w_0=...$.

2. Repeat
$$w \leftarrow w + \Delta w $$ for 1000 steps. Set $\eta = 0.1$.

3. If you plot the function within a range of -4 and 12, you can see the shape the best(this is important: otherwise it is a bit hard to interprete the results). See the example below and the sample code.  

4. Try different initial value $w_0$ and confirm that the final $w$ values are the same for those that end up in the same local/global minumum. Give two examples $w$ values that lead to a local minumum, and two that lead to the global minumum.

Answer: local minimum: 1, 4; global minimum: 6, 8
"""

# Implement your program here:
import numpy as np
import matplotlib.pyplot as plt
# Example program. You can modity/extend this.
#
# 1. define function 
# - this is an example (quadratic function) -- you should change it to the function given in problem 4)
# - also, you need to check for the termination condition, rather than using a fixed number of steps.
def E(w):
  return 1/40*(np.power(w,4) - 17*np.power(w,3) + 62*np.power(w,2) + 80*w)      # note: np.power() can take scalar or vector as argument.

# 2. define derivative function 
def dEdw(w):
  return 1/10*np.power(w,3) - 51/40*np.power(w,2) + 31/10*w + 2

# 3. gradient descent 

w = 5     # initial value
eta = 0.1   # learning rate  : try eta = 0.2 , and eta=0.001

n = 1000

w_t = np.zeros(n)
Ew_t = np.zeros(n)

end_idx = 0
for i in range(n):

  # collect w_t and Ew_t values for plotting
  w_t[i]=w
  Ew_t[i]=E(w)
  
  end_idx=i # book keeping

  # IMPLEMENT the line below: gradient descent step
  DeltaW = -eta * dEdw(w)
  w = w + DeltaW   

# 4. plot E(w) and the gradient descent steps 
x = np.linspace(-4,12,50)
# For the problem 4 function, use 
# x = np.linspace(-4,12,50)

# Note: initially, this will only show one point, at w=0.5.
plt.plot(x,E(x),"-",w_t[0:end_idx],Ew_t[0:end_idx],"*-")

"""**Problem 8 (Written: 5 pts):**
Using chain rule, find the gradient for the function $\sigma(E(w))$ where
$\sigma(x) = \frac{1}{1+\exp(-x)}$ and $E(w)$ is defined above (Problem 4):
$$\frac{d\sigma(E(w))}{dw}.$$
Write the answer as a function of $w$. Do not use Wolfram Alpha or similar package (you'll get a mathematically equivalent equation but it will be much more complex). If you do that you will get no credit.

**Hint:** $\frac{d\sigma(x)}{dx} = \sigma(x)(1-\sigma(x))$.

**Answer:**

$\frac{d \sigma(E(w))}{dw} = \frac{\exp(-E(w))}{(1+\exp(-E(w))^2}(\frac{1}{10}w^3-\frac{51}{40}w^2+\frac{31}{10}w+2) $

# Section IV. Backpropagation 1 (25 pt)

**Problem 9 (Program: 15 pts):**

For this problem, use Keras with TensorFlow to train the data set shown in Problem 3 from scratch. Experiment with different network configurations, and report the results. 

(1) Single layer network;

(2) Two layer networks: Try 2 hidden units, 3 hidden units, and 4 hidden units. In all cases, output unit is 1. 

(3) Analyze the weights and plot the decision boundary (youâ€™ll get a bunch of line equations). Or, alternatively, you can plot the activation value of each hidden or output unit (if you had 3 hidden and 1 output unit, you will need to show 4 such plots in total). For the input, generate a mesh grid of finer grain than the grid in the  Problem 3 data set (see python example below), and then compute the hidden activity.
"""

# This shows how to create the input vectors in a systematic manner, for testing. Suppose a 
# well trained neural network has one hidden layer. This hidden layer has two
# neurons: weights [w_1, W2]= [[1, 1], [-1, 1]], biases [b_1, b_2] = [-0.3, -0.5]. 
# We want to plot decision boundaries for hidden neurons' output >= 0.
import numpy as np
import matplotlib.pyplot as plt

# generate a input meshgrid
x = np.arange(-0.1, 5.1, 0.1)
y = np.arange(-0.1, 5.1, 0.1)
xx, yy = np.meshgrid(x, y)   # xx and yy are matrices 

# note: for simplicity, here it is assumed that the hidden layer uses linear activation function.
w11, w12, b1 = 1, 1, -0.3
w21, w22, b2 = -1, 1, -0.5
h1 = w11 * xx + w12 * yy + b1
h2 = w21 * xx + w22 * yy + b2

# plot using imshow : with thresholding 
# - be careful with x-y range (extent = ...) and the orientation (origin = ...)

thresh = 0

fig1 = plt.figure(figsize=[10,4])
ax1 = plt.subplot(1, 2, 1) # 1 row 2 columns, index1
ax2 = plt.subplot(1, 2, 2) # index2

plt.sca(ax1)
plt.imshow(h1 > thresh, extent=[-0.1, 5.1, -0.1, 5.1], origin='lower') 

plt.sca(ax2)
plt.imshow(h2 > thresh, extent=[-0.1, 5.1, -0.1, 5.1], origin='lower')

# plot, without thresholding 

fig2 = plt.figure(figsize=[10,4])
ax3 = plt.subplot(1, 2, 1)
ax4 = plt.subplot(1, 2, 2)

plt.sca(ax3)
plt.imshow(h1, extent=[-0.1, 5.1, -0.1, 5.1], origin='lower') 

plt.sca(ax4)
plt.imshow(h2, extent=[-0.1, 5.1, -0.1, 5.1], origin='lower')

import numpy as np
import keras
import tensorflow as tf
from keras.models import Sequential
from keras.layers import Dense
from keras.models import Model

# config
eta  = 0.007          # learning rate 
num_epochs = 3000
b_size = 15   # batch size

# data
x_train = np.array([[0,0],[0,1],[0,2],[0,3],[0,4],[1,0],[1,1],[1,2],[1,3],[1,4],[2,0],[2,1],[2,2],[2,3],[2,4],[3,0],[3,1],[3,2],[3,3],[3,4],[4,0],[4,1],[4,2],[4,3],[4,4]])
y_train = np.array([0,0,0,0,0,0,0,1,0,0,0,1,1,1,0,0,0,1,0,0,0,0,0,0,0])
x_test = x_train
y_test = y_train

# define model architecture
model = Sequential()
model.add(Dense(1, activation='sigmoid'))  # add output layer 

# optimizer
opt = tf.keras.optimizers.Adam(learning_rate=eta)


# build model 
model.compile(loss='mean_squared_error',  # loss defines the error function E()
		optimizer=opt,
		metrics=['accuracy']) # metrics is the performance metric: may use 'accuracy' but the results will be against your intuition

# train & evaluate 
model.fit(x_train,y_train, epochs=num_epochs, batch_size=b_size)
score = model.evaluate(x_test, y_test, batch_size=b_size)

# retrieve results 
x_test = x_train

final = Model(inputs=model.input, outputs=model.layers[0].output)
final_output = final.predict(x_test)

print(final_output)

import numpy as np
import keras
import tensorflow as tf
from keras.models import Sequential
from keras.layers import Dense
from keras.models import Model

# config
eta  = 0.007          # learning rate 
num_epochs = 3000
b_size = 15   # batch size

# data
x_train = np.array([[0,0],[0,1],[0,2],[0,3],[0,4],[1,0],[1,1],[1,2],[1,3],[1,4],[2,0],[2,1],[2,2],[2,3],[2,4],[3,0],[3,1],[3,2],[3,3],[3,4],[4,0],[4,1],[4,2],[4,3],[4,4]])
y_train = np.array([0,0,0,0,0,0,0,1,0,0,0,1,1,1,0,0,0,1,0,0,0,0,0,0,0])
x_test = x_train
y_test = y_train

# define model architecture
model = Sequential()
model.add(Dense(2, input_dim=2, activation='tanh'))   # add hidden layer
model.add(Dense(1, activation='sigmoid'))  # add output layer 

# optimizer
opt = tf.keras.optimizers.Adam(learning_rate=eta)


# build model 
model.compile(loss='mean_squared_error',  # loss defines the error function E()
		optimizer=opt,
		metrics=['accuracy']) # metrics is the performance metric: may use 'accuracy' but the results will be against your intuition

# train & evaluate 
model.fit(x_train,y_train, epochs=num_epochs, batch_size=b_size)
score = model.evaluate(x_test, y_test, batch_size=b_size)

# retrieve results 
x_test = x_train

final = Model(inputs=model.input, outputs=model.layers[1].output)
final_output = final.predict(x_test)

print(final_output)

import numpy as np
import keras
import tensorflow as tf
from keras.models import Sequential
from keras.layers import Dense
from keras.models import Model

# config
eta  = 0.007          # learning rate 
num_epochs = 3000
b_size = 15   # batch size

# data
x_train = np.array([[0,0],[0,1],[0,2],[0,3],[0,4],[1,0],[1,1],[1,2],[1,3],[1,4],[2,0],[2,1],[2,2],[2,3],[2,4],[3,0],[3,1],[3,2],[3,3],[3,4],[4,0],[4,1],[4,2],[4,3],[4,4]])
y_train = np.array([0,0,0,0,0,0,0,1,0,0,0,1,1,1,0,0,0,1,0,0,0,0,0,0,0])
x_test = x_train
y_test = y_train

# define model architecture
model = Sequential()
model.add(Dense(3, input_dim=2, activation='tanh'))   # add hidden layer
model.add(Dense(1, activation='sigmoid'))  # add output layer 

# optimizer
opt = tf.keras.optimizers.Adam(learning_rate=eta)


# build model 
model.compile(loss='mean_squared_error',  # loss defines the error function E()
		optimizer=opt,
		metrics=['accuracy']) # metrics is the performance metric: may use 'accuracy' but the results will be against your intuition

# train & evaluate 
model.fit(x_train,y_train, epochs=num_epochs, batch_size=b_size)
score = model.evaluate(x_test, y_test, batch_size=b_size)

# retrieve results 
x_test = x_train

final = Model(inputs=model.input, outputs=model.layers[1].output)
final_output = final.predict(x_test)

print(final_output)

import numpy as np
import keras
import tensorflow as tf
from keras.models import Sequential
from keras.layers import Dense
from keras.models import Model

# config
eta  = 0.007          # learning rate 
num_epochs = 3000
b_size = 15   # batch size

# data
x_train = np.array([[0,0],[0,1],[0,2],[0,3],[0,4],[1,0],[1,1],[1,2],[1,3],[1,4],[2,0],[2,1],[2,2],[2,3],[2,4],[3,0],[3,1],[3,2],[3,3],[3,4],[4,0],[4,1],[4,2],[4,3],[4,4]])
y_train = np.array([0,0,0,0,0,0,0,1,0,0,0,1,1,1,0,0,0,1,0,0,0,0,0,0,0])
x_test = x_train
y_test = y_train

# define model architecture
model = Sequential()
model.add(Dense(4, input_dim=2, activation='tanh'))   # add hidden layer
model.add(Dense(1, activation='sigmoid'))  # add output layer 

# optimizer
opt = tf.keras.optimizers.Adam(learning_rate=eta)


# build model 
model.compile(loss='mean_squared_error',  # loss defines the error function E()
		optimizer=opt,
		metrics=['accuracy']) # metrics is the performance metric: may use 'accuracy' but the results will be against your intuition

# train & evaluate 
model.fit(x_train,y_train, epochs=num_epochs, batch_size=b_size)
score = model.evaluate(x_test, y_test, batch_size=b_size)

# retrieve results 
x_test = x_train

final = Model(inputs=model.input, outputs=model.layers[1].output)
final_output = final.predict(x_test)

print(final_output)

# Generate mesh of (x,y) values to test.
# - although we used only 0 and 1 in training, we can plug in input in the full 0:1 range.
import numpy as np
import matplotlib.pyplot as plt

# generate a input meshgrid
x = np.arange(-0.1, 5.1, 0.1)
y = np.arange(-0.1, 5.1, 0.1)
xx, yy = np.meshgrid(x, y)   # xx and yy are matrices 

n=xx.size   ## xx and yy are meshgrids defined above, right below Problem 11 description. 

mesh_input = np.concatenate((xx.reshape(n,1),yy.reshape(n,1)),1)  # construct n x 2 array as input.

# Retrieve hidden layer activity

layer_to_show = 0       # 0 = first hidden, 1 = second hidden (or output, depending on the model architecture), etc. 

final = Model(inputs=model.input, outputs=model.layers[layer_to_show].output)
final_output = final.predict(mesh_input)  # this includes all hidden layer activities

# first hidden unit: final_output[0:,0] retrieves all 1st hidden unit activation values 
plt.matshow(final_output[0:,0].reshape(52,52),extent=[0,5,0,5], origin='lower')  
plt.show()

# second hidden unit: similar to the above 
thresh = 0
plt.matshow(final_output[0:,1].reshape(52,52),extent=[0,5,0,5], origin='lower')
plt.show()

# third hidden unit: similar to the above 
thresh = 0
plt.matshow(final_output[0:,2].reshape(52,52),extent=[0,5,0,5], origin='lower')
plt.show()

# fourth hidden unit: similar to the above 
thresh = 0
plt.matshow(final_output[0:,3].reshape(52,52),extent=[0,5,0,5], origin='lower')
plt.show()

# output layer: See if the decision boundary follows your intuition. Try it without the thresholding too.
layer_to_show = 1      # since we have hidden layer and output layer, output layer number is 1 

final = Model(inputs=model.input, outputs=model.layers[layer_to_show].output)
final_output = final.predict(mesh_input)

plt.matshow(final_output.reshape(52,52),extent=[0,5,0,5], origin='lower')
plt.show()

# Try thresholding the final_output vlaues ! 
# plt.matshow(final_output.reshape(52,52)>thresh,extent=[0,5,0,5], origin='lower')

"""**Answer:**

(1) The single layer network can't classify any point, all points are between 0.1 and 0.2.

(2) The two layer network with 2 hidden unit can't classify any point.

The 3 hidden unit gives the class 1 points higher value, which is larger than 0.4. However, some class 0 points also has value larger than 0.4.

The 4 hidden unit network classifies the data correctly. If we set a threshold of 0.8, all points will be correct.

(3) For the 4 hidden unit network: The last hidden unit has a boundary at the most lower left, which is not helpful. The other three units form a triangle, which manages to classify all data

**Problem 10 (Program: 10 pts):** Again, use Keras with TensorFlow. Implement sine function approximation from scratch. The training and test data are generated as below.

(1) Experiment with different network configurations: Two layers (5 hidden units, 10 hidden units, 30 hidden units), Three layers (1st hidden layer: 5 hidden units, 2nd hidden layer: 3 hidden units  ; and   1st hidden layer: 20 units, 2nd: 3 units).

(2) Plot each unit's activities in the last hidden layers (right before the output layer) in each of your models. Explain how the different number of hidden layers and number of hidden units affect the accuracy of the model, and how the hidden representations contribute to the final outcome.

(3) Run your models by slightly extending the original test set (for example, input from $[-6\pi, 6\pi]$, so that the x-range goes beyond that of the training set). Explain why the out-of-range portions behave in the way that you observe.
"""

# Important Note: 
#
# In this case, "x" is the input and "y" is the target value! 
#
# Do not confuse this with the classification case, where you had "(x,y)"" as input
# and "c" as target value.
import numpy as np
import matplotlib.pyplot as plt

x_train = np.arange(-2*np.pi, 2*np.pi, 0.1)
y_train = np.sin(x_train)
x_test = x_train
y_test = y_train
x_extend = np.arange(-6*np.pi, 6*np.pi, 0.1)
y_extend = np.sin(x_extend)

plt.figure(figsize=[12, 4])
ax1 = plt.subplot(1, 2, 1)
ax2 = plt.subplot(1, 2, 2)

plt.sca(ax1)
plt.plot(x_train, y_train, marker='.', color='r', linestyle='-')
plt.grid()

plt.sca(ax2)
plt.plot(x_extend, y_extend, marker='.', color='b', linestyle='-')
plt.grid()

import numpy as np
import keras
import tensorflow as tf
from keras.models import Sequential
from keras.layers import Dense
from keras.models import Model
import matplotlib.pyplot as plt

x_train = np.arange(-2*np.pi, 2*np.pi, 0.1)
y_train = np.sin(x_train)
x_test = x_train
y_test = y_train

# config
eta  = 0.007          # learning rate 
num_epochs = 500
b_size = 2   # batch size

# define model architecture
model = Sequential()
model.add(Dense(20, input_dim=1, activation='tanh'))   # add hidden layer
model.add(Dense(3, input_dim=1, activation='tanh'))
model.add(Dense(1, activation='tanh'))  # add output layer 

# optimizer
opt = tf.keras.optimizers.Adam(learning_rate=eta)


# build model 
model.compile(loss='mean_squared_error',
              optimizer=tf.keras.optimizers.Adam(0.001),
              metrics=[tf.keras.metrics.MeanAbsoluteError()])

# train & evaluate 
model.fit(x_train,y_train, epochs=num_epochs, batch_size=b_size)
score = model.evaluate(x_test, y_test, batch_size=b_size)

# retrieve results 
#x_test = x_extend

final = Model(inputs=model.input, outputs=model.layers[2].output)
final_output = final.predict(x_test)

print(final_output)

plt.figure(figsize=[12, 4])
ax1 = plt.subplot(1, 2, 1)
ax2 = plt.subplot(1, 2, 2)

plt.sca(ax1)
plt.plot(x_test, final_output, marker='.', color='r', linestyle='-')
plt.grid()

plt.sca(ax2)
plt.plot(x_test, y_test, marker='.', color='b', linestyle='-')
plt.grid()

import numpy as np
import keras
import tensorflow as tf
from keras.models import Sequential
from keras.layers import Dense
from keras.models import Model
import matplotlib.pyplot as plt

x_train = np.arange(-2*np.pi, 2*np.pi, 0.1)
y_train = np.sin(x_train)
x_test = x_train
y_test = y_train

# config
eta  = 0.007          # learning rate 
num_epochs = 500
b_size = 2   # batch size

# define model architecture
model = Sequential()
model.add(Dense(20, input_dim=1, activation='tanh'))   # add hidden layer
model.add(Dense(3, input_dim=1, activation='tanh'))
model.add(Dense(1, activation='tanh'))  # add output layer 

# optimizer
opt = tf.keras.optimizers.Adam(learning_rate=eta)


# build model 
model.compile(loss='mean_squared_error',
              optimizer=tf.keras.optimizers.Adam(0.001),
              metrics=[tf.keras.metrics.MeanAbsoluteError()])

# train & evaluate 
model.fit(x_train,y_train, epochs=num_epochs, batch_size=b_size)
score = model.evaluate(x_test, y_test, batch_size=b_size)

# retrieve results 
#x_test = x_extend

final = Model(inputs=model.input, outputs=model.layers[1].output)
final_output = final.predict(x_test)

print(final_output)

plt.figure(figsize=[12, 4])
ax1 = plt.subplot(1, 2, 1)
ax2 = plt.subplot(1, 2, 2)

plt.sca(ax1)
plt.plot(x_test, final_output, marker='.', color='r', linestyle='-')
plt.grid()

plt.sca(ax2)
plt.plot(x_test, y_test, marker='.', color='b', linestyle='-')
plt.grid()

import numpy as np
import keras
import tensorflow as tf
from keras.models import Sequential
from keras.layers import Dense
from keras.models import Model
import matplotlib.pyplot as plt

x_train = np.arange(-2*np.pi, 2*np.pi, 0.1)
y_train = np.sin(x_train)
x_extend = np.arange(-6*np.pi, 6*np.pi, 0.1)
y_extend = np.sin(x_extend)
x_test = x_extend
y_test = y_extend

# config
eta  = 0.007          # learning rate 
num_epochs = 500
b_size = 2   # batch size

# define model architecture
model = Sequential()
model.add(Dense(20, input_dim=1, activation='tanh'))   # add hidden layer
model.add(Dense(3, input_dim=1, activation='tanh'))
model.add(Dense(1, activation='tanh'))  # add output layer 

# optimizer
opt = tf.keras.optimizers.Adam(learning_rate=eta)


# build model 
model.compile(loss='mean_squared_error',
              optimizer=tf.keras.optimizers.Adam(0.001),
              metrics=[tf.keras.metrics.MeanAbsoluteError()])

# train & evaluate 
model.fit(x_train,y_train, epochs=num_epochs, batch_size=b_size)
score = model.evaluate(x_test, y_test, batch_size=b_size)

# retrieve results 
#x_test = x_extend

final = Model(inputs=model.input, outputs=model.layers[2].output)
final_output = final.predict(x_test)

print(final_output)

plt.figure(figsize=[12, 4])
ax1 = plt.subplot(1, 2, 1)
ax2 = plt.subplot(1, 2, 2)

plt.sca(ax1)
plt.plot(x_test, final_output, marker='.', color='r', linestyle='-')
plt.grid()

plt.sca(ax2)
plt.plot(x_test, y_test, marker='.', color='b', linestyle='-')
plt.grid()

"""**Answer:**

**(1)**

1 hidden layers: 

5 units: First crest and last trough are not complete.

10 units: First crest and last trough are not complete.

30 units: First crest and last trough are not complete.

2 hidden layers:

5 units + 3 units: First crest and last trough are not complete.

20 units + 3 units: This network can learn the sine function mostly correct, except some peak values are a little smallet than 1, and minimum value is a little larger than -1.

**(2)** 

1 hidden layers: 

5 units: 3 units are similar to tanh functions, and 2 units is similar to lines (almost straight) that passes (0,0) with slope 1 and -1, therefore the result is inaccurate.

10 units: Some units are similar to lines (almost straight) that passes (0,0) with slope 1 and -1, some are similar to tanh and opposite of tanh, and some are flat lines on x axis.

30 units: Similar to 10 units, but has more units. 

2 hidden layers:

5 units + 3 units: The last hidden layer has 1 curve similar to the reflection along y axis of tanh function, and 2 units are wave curves which has small peak values.

20 units + 3 units: The last hidden layer has 1 curve similar to tanh function, and 2 more complex curves, therefore it can combine those units and represent a sine function

**(3)**

The best network(3 layers, 20 + 3 hidden units) can learn the range between 2pi and -2pi correctly, however, the network gives tanh function (decreases to -1 and stays flat for range < -2pi, increases to 1 and stay flat for the range > 2pi) outside of this range. This is because there is no data samples outside 2pi and -2pi, therefore the network can't use gradient descent (no xi) and backpropagation to learn the correct values.

# Section V. Backpropagation 2 (20 pt)

**Problem 11 (Program: 20 pt)** 

Build a multilayer neural network to learn the spiral data set below. Use as many layers as needed. 

(1) Write the code, using the code above (example code for XOR).

(2) Design an appropriate network architecture (number of layers, number of units in each layer, activation function type[link text](https://) [first try tanh]), and train the network to reach high accuracy (> 95%). Note: There will be only one output unit, which outputs +1 for positive, and -1 for negative. 

(3) Plot the decision boundary of all the hidden units and the output unit (write code to do this, for arbitrary number of layers and neurons). Hint: see https://playground.tensorflow.org for example. No need to plot the connections. Just plot 1st hidden layer in the first row, 2nd hidden layer in the second row, etc. See example below:

```
[  ] [  ] [  ] [  ]  : hidden layer 1 boundaries
[  ] [  ] [  ]       : hidden layer 2 boundaries
...
...
[  ] : output layer
```

(4) Explain how adding more layers allows the multilayer network to learn more complex decision boundaries, referring to your answer to (3) above.

Ans: Deeper layers can generate more complex boundaries by combining the output of previous layers, therefore it can eventually output a boundary that can divide complex data.
"""

''' 
Spiral data set
'''

import random
import numpy as np
import keras
import tensorflow as tf
from keras.models import Sequential
from keras.layers import Dense
from keras.models import Model
import matplotlib.pyplot as plt

# config
rand_factor = 1.5

#--------------------
def spiral_1(t):
#--------------------
  '''
  parametric curve of a spiral : clockwise
  '''
  dat   = np.array([(t+1)*np.sin(t+5), (t+1)*np.cos(t+5)])
  noise = np.random.rand(2,len(dat[0,:]))*rand_factor
  return (dat+noise-0.5)/7.5


#--------------------
def spiral_2(t):
#--------------------
  '''
  parametric curve of a spiral : counter-clockwise
  '''
  dat = np.array([-(t+1)*np.sin(t+5) ,-(t+1)*np.cos(t+5)])
  noise = np.random.rand(2,len(dat[0,:]))*rand_factor
  return (dat+noise-0.5)/7.5

#---------------------
# DATA: 
#  pos_dat is positive data (output = +1)
#  neg_dat is negative data (output = +1)
#
#  Note: pos_dat([:,0]) is the input x values (column vector), and
#        pos_dat([:,1]) is the input y values (column vector).
#        pos_dat([k,:]) is the single sample (x_k, y_k), and the target 
#                       value would be "+1" (since this is the positive set)
#---------------------

# Set data range
t = np.arange(0, 2*np.pi, 0.05)

pos_dat = np.transpose(spiral_1(t))
neg_dat = np.transpose(spiral_2(t))

#---------------------
# Plot the data: 
#   plotting all rows:
#--------------------- each row is the x,y coordinate
plt.plot(pos_dat[:,0],pos_dat[:,1],"o",neg_dat[:,0],neg_dat[:,1],"x")


''' 
IMPLEMENT your neural network below
'''

# to construct the data set, first you can concatenate pos_dat and neg_dat, 
# and then set the target value vector to [1,1,1,1,1,1, .... ,1,-1,-1,-1,-1,-1, .... , -1].


# config
eta  = 0.03          # learning rate 
num_epochs = 1300
b_size = 10   # batch size

# data
pos_size = int(pos_dat.size/2)
neg_size = int(neg_dat.size/2)
print(pos_size)
print(neg_size)
target = []
for i in range(pos_size):
  target.append(1)
for i in range(neg_size):
  target.append(-1)

x_train = np.concatenate((pos_dat, neg_dat), axis=0)
y_train = np.array(target)
x_test = x_train
y_test = y_train

# define model architecture
model = Sequential()
model.add(Dense(8, input_dim=2, activation='tanh'))   # add hidden layer
model.add(Dense(4, input_dim=2, activation='tanh'))   # add hidden layer
model.add(Dense(2, input_dim=2, activation='tanh'))   # add hidden layer
model.add(Dense(1, activation='tanh'))  # add output layer 

# optimizer
opt = tf.keras.optimizers.Adam(learning_rate=eta)


# build model 
model.compile(loss='mean_squared_error',  # loss defines the error function E()
		optimizer=opt,
		metrics=['accuracy']) # metrics is the performance metric: may use 'accuracy' but the results will be against your intuition

# train & evaluate 
model.fit(x_train,y_train, epochs=num_epochs, batch_size=b_size)
score = model.evaluate(x_test, y_test, batch_size=b_size)

# retrieve results 
x_test = x_train

final = Model(inputs=model.input, outputs=model.layers[3].output)
final_output = final.predict(x_test)

correct = 0
for i in range (pos_size):
  if final_output[i] > 0.9: correct = correct+1
for i in range (pos_size, 2*pos_size):
  if final_output[i] < -0.9: correct = correct+1
print("\nPrediction Accuracy: {0:f}\n".format(correct/(2*pos_size)))
print(final_output)

x = np.arange(-1, 1, 0.1)
y = np.arange(-1, 1, 0.1)
xx, yy = np.meshgrid(x, y)   # xx and yy are matrices 

n=xx.size   ## xx and yy are meshgrids defined above, right below Problem 11 description. 

mesh_input = np.concatenate((xx.reshape(n,1),yy.reshape(n,1)),1)  # construct n x 2 array as input.

# Retrieve hidden layer activity
layer_to_show = 0       
final = Model(inputs=model.input, outputs=model.layers[layer_to_show].output)
final_output = final.predict(mesh_input)  # this includes all hidden layer activities
fig1 = plt.figure(figsize=[10,4])

for i in range(8):
  plt.sca(plt.subplot(1, 8, i+1))
  plt.imshow(final_output[0:,i].reshape(20,20),extent=[-1,1,-1,1], origin='lower')

layer_to_show = 1       
final = Model(inputs=model.input, outputs=model.layers[layer_to_show].output)
final_output = final.predict(mesh_input)  # this includes all hidden layer activities 
fig2 = plt.figure(figsize=[10,4])

for i in range(4):
  plt.sca(plt.subplot(1, 4, i+1))
  plt.imshow(final_output[0:,i].reshape(20,20),extent=[-1,1,-1,1], origin='lower')

layer_to_show = 2       
final = Model(inputs=model.input, outputs=model.layers[layer_to_show].output)
final_output = final.predict(mesh_input)  # this includes all hidden layer activities
fig3 = plt.figure(figsize=[10,4])

for i in range(2):
  plt.sca(plt.subplot(1, 2, i+1))
  plt.imshow(final_output[0:,i].reshape(20,20),extent=[-1,1,-1,1], origin='lower') 



# output layer: See if the decision boundary follows your intuition. Try it without the thresholding too.
layer_to_show = 3     
final = Model(inputs=model.input, outputs=model.layers[layer_to_show].output)
final_output = final.predict(mesh_input)

plt.matshow(final_output.reshape(20,20),extent=[-1,1,-1,1], origin='lower')
plt.show()